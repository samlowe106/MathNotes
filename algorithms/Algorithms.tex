\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts} % \mathbb
\usepackage{mathtools}
\usepackage{latexsym}
\usepackage{amssymb} % therefore
\usepackage{hyperref} % allows hyperlinks, including in the table of contents
\usepackage{amsthm}
%\usepackage[document]{ragged2e} % left justifies text

\DeclareMathOperator{\Div}{Div}
\DeclareMathOperator{\Curl}{Curl}

\DeclareMathOperator{\dom}{dom}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}

\title{Algorithms Notes}
\author{Sam Lowe}

\begin{document}

\maketitle

\tableofcontents

\pagebreak

A \textbf{perfect matching} is


\underline{Lecture 2}

A matching $M$ is a non-empty set of doctor-hospital pairs where no doctor/hospital appears twice. A matching is called \textbf{perfect} if every doctor/hospital appears once.

We say ``$d$ is matched to $h$" if $(d, m) \in M$

We say ``$d$ is matched" if $(d, m) \in M$ for some $h$

We say a matching is \textbf{unstable} if some doctor/hospital pair prefer one another to their mate in $M$. Examples: \begin{enumerate}
    \item $d, h$ such that d is matched to h', h is unmatched, but d prefers h to h'
    \item d, h such that h is matched to d', d is unmatched, but h prefers d to d'
    \item d, h such that d is matched to h', h is matched to d', but d prefers h to h' and h prefers d to d'
\end{enumerate}

If a matching is perfect and not unstable, we call it \textbf{stable}.

\textbf{Gale-Shapley}

\begin{enumerate}
    \item Let M be empty
    \item While (some hospital h is unmatched): \begin{enumerate}
        \item If (h has offered a job to everyone): break
        \item Else: let d be the highest-ranked doctor to which h has not yet offered a job
        \item h makes offer to d: \begin{enumerate}
            \item If (d is unmatched): d accepts, add (d, h) to M
            \item ElseIf (d is matched to h' and d prefers h' to h): d rejects, pass
            \item ElseIf (d is matched to h' and d prefers h to h'): then d accepts, remove (d,h') from M and add (d,h) to M
        \end{enumerate}
    \item Return M
    \end{enumerate}
\end{enumerate}

\underline{Lecture 3}

Observations for Gale-Shapley:

\begin{enumerate}
    \item Hospitals make offers in descending order. (Hospitals' happiness decreases throughout the course of the algorithm.)
    \item Doctors that get a job never become unemployed. (Doctors never become unmatched once matched.)
    \item Doctors accept offers in ascending order. (Doctors happiness increases throughout the course of the algorithm.)
\end{enumerate}

Questions about Gale-Shapley (when the number of doctors = the number of hospitals): \begin{enumerate}
    \item Does Gale-Shapley always terminate? How long does it take?
          Yes, Gale-Shapley terminates after at most $n^2$ iterations ($n^2$ job offers). Each hospital can offer each doctor a job at most one time. We know that the number of doctors is at most the number of hospitals, so the number of offers is $\leq n^2$.
    \item Does Gale-Shapley output a perfect matching?
          Yes. Assume that Gale-Shapley fails to return on a perfect matching. Then some doctor there's some doctor $d$ who is not matched and some hospital $h$ which is also not matched.
          $h$ offered a job to $d$ and: \begin{enumerate}
              \item $d$ accepts $\to$ $d$ must stay employed
              \item $d$ rejects $to$ doctors reject only if paired because doctors stay employed
          \end{enumerate}
    \item Does GS output a stable matching?
          Yes. Suppose there's an instability $(d, h'), (d', h)$ and $d$ prefers $h$ to $h'$ and $h$ prefers $d$ to $d'$, then we know that $h$ must've made an offer to $d$ before $d'$ by observation 1. \begin{enumerate}
            \item If $d$ was matched with $h'$, then $d$ must prefer $h'$ to $h$ by observation 3, which is a contradiction.  
            \item Case 2: $d$ accepted the offer - $(h, h) \in M$ at some point by observation 3; doctors must get happier. $d: h' > h$.
          \end{enumerate}
          
          
          Each offer uses roughly $n$ operations in the worst-case.
          Finding $h$ in $d$'s preference list (in the ElseIf) could take, at worst, $n$ operations. 
\end{enumerate}

\underline{Lecture 4}

\underline{Asymptotic Analysis}

\begin{enumerate}
    \item Tool used to compare algorithms (functions).
    \item Described performance based on input size (n).
    \item Measures speed/size as input grows; focuses on dominant (largest) term of function.
\end{enumerate}

Predicting the wall-clock time of an algorithm is nearly impossible. What machine will actually run the algorithm? It's impossible to exactly time operations due to inconsistencies in call times; what's the margin of error?

Exact running time (# of operations), represented by $f(n)$. This function would be pretty messy.

Order of growth $g(n)$: a nicer function that summarizes performance.

\textbf{Definition: (Big-Oh notation)}: We write $f(n) = O(g(n))$ if there exists $c \in (0, \infty)$ and $n_0 \in N$ such that $f(n) \leq c \cdot g(n)$ for every $g \geq n_0$. \begin{itemize}
    \item Asymptotic version of $f(n) \leq g(n)$
    \item Roughly equivalent to $\lim_{n \to \infty} \frac{f(n)}{g(n)} < \infty$
\end{itemize}

\textbf{Example} $3n^2 + n = O(n^2)$.

WTS $3n^2 + n \leq c \cdot n^2$ for all $n \geq n_0$.

Strategy: sum coefficients of $f(n)$. In this case, we have $3 + 1 = 4$. So we have:

$3n^2 + n \leq 4n^2$ for all $n \geq 1$.

$n \leq n^2$ for all $n \geq 1$.

\underline{Lecture 4}

Theorems for Asymptotic Analysis:

(Order of growth): \begin{enumerate}
    \item Constant factors can be ignored because $\forall C > 0$, $Cn = O(n)$
    \item Lower order terms can be dropped; for example, $\forall n^2 + n^{3/2} = O(n^2)$
\end{enumerate}

(Comparing/Ranking): \begin{enumerate}
    \item Smaller exponents are Big-Oh of larger exponents
    \item Any logarithm is Big-Oh of any polynomial
    \item Any polynomial is Big-Oh of any exponential
\end{enumerate}

\textbf{Definition (Big Omega Notation)} $f(n) = \Omega(g(n))$ if there exists $c \in (0, \infty)$ and $n_0 \in \mathbb{N}$ such that $f(n) \geq c \cdot g(n)$ for every $n \geq n_0$.

Note: this is roughly equivalent to saying $\lim_{n \to \infty} \frac{f(n)}{g(n)} > 0$, and the ``asymptotic equivalent'' of $f(n) \geq g(n)$

\textbf{Definition (Big Theta Notation)} $f(n) = \Theta(g(n))$ if there exists $c_1 \leq c_2 \in (0, \infty)$ and $n_0 \in \mathbb{N}$ such that $c_2 * g(n) \geq f(n) \geq c_1 g(n)$ for every $n \geq n_0$.

Note: this is roughly equivalent to saying $\lim_{n \to \infty} \frac{f(n)}{g(n)} \in (0, \infty)$, and the ``asymptotic equivalent'' of $f(n) = g(n)$, and equivalent to $f(n) = O(g(n))$ AND $f(n) = \Omega(g(n))$

It's nice to write running time as Big-Theta; it's more precise than Big-Oh or Big-Omega, but it's often simpler to just use Big-Oh notation

\textbf{Definition: (Little-Oh notation)}: We write $f(n) = o(g(n))$ if there exists $c \in (0, \infty)$ and $n_0 \in N$ such that $f(n) < c \cdot g(n)$ for every $n \geq n_0$. \begin{itemize}
    \item Asymptotic version of $f(n) < g(n)$
    \item Roughly equivalent to $\lim_{n \to \infty} \frac{f(n)}{g(n)} = 0$
\end{itemize}

\textbf{Definition (Little-Omega Notation)} $f(n) = \omega(g(n))$ if there exists $c \in (0, \infty)$ and $n_0 \in \mathbb{N}$ such that $f(n) > c \cdot g(n)$ for every $n \geq n_0$.

Note: this is roughly equivalent to saying $\lim_{n \to \infty} \frac{f(n)}{g(n)} = \infty$, and the ``asymptotic equivalent'' of $f(n) > g(n)$

\underline{Lecture 5}

Selection sort: given $n$ elements, find the minimum, swap it with the first element, then recursively repeat the algorithm on last $n-1$ elements.

Counting operations: \begin{enumerate}
    \item Creating (and assigning a value to) a variable
    \item Assigning a value to an existing variable
    \item Looking up a value
    \item arithmetic
    \item Comparisons
    \item Function calls
\end{enumerate} All of the above are operations.

Divide and conquer: Mergesort

Given a list of $n > 2$ elements, divide the list in half and recursively sort, then merge the lists back together.

Idea: If $L, R$ are sorted lists of length $n / 2$, we can merge them into a sorted list $A$ of length $n$ in time $\Theta(n)$. (Merging two sorted lists is faster than sorting by scratch!)

Mergesort(A): \begin{enumerate}
    \item If (len(A) = 1): Return A      // Base case
    \item Let m := ceil(Len(A) / 2)      // Split
    \item Let L := A[1:m]
    \item Let R := A[m+1:n]
    \item Let L := Mergesort(L)          // L is sorted
    \item Let R := Mergesort(R)          // R is sorted
    \item Let A := Merge(L, R)
    \item Return A
\end{enumerate}

Summary of Mergesort: \begin{itemize}
    \item Sorts a list of $n$ numbers in $\Theta(n\log n)$ time
    \item No comparison based algorithm can be (much) faster
    \item Merging sorted lists is easier than sorting
    \item Proof of correctness by induction
    \item Analysis of running time done by recurrences and recursion trees
\end{itemize}


\underline{Lecture 6}

How long does integer addition take? $O(n)$

How long does integer multiplication take? $O(n^2)$

We have $n^2$ digits to multiply together, then we have to add up each of those $n$ products together


Divide and Conquer multiplication

Where $a$ is the first $n / 2$ digits and $b$ is the last $n / 2$ digits of a number (and likewise for $cd$): 

$\left(a * 10^{n / 2} + b\right) left(c * 10^{n / 2} + d\right) = ab * cd$ (where $ab$ is $a$ and $b$ concatenated, likewise for $cd$)

$= ac10^n + (ad + bc)10^{n / 2} + bd$

We're doing $n / 2$ multiplications; four $n / 2$-digit multiplications, three $n$-digit adds (each of which are $O(n)$) and some shifts $(n)$

Recurrence: $T(n) = 4T\left(\frac{n}{2}\right)+ \Theta (n)$

\textbf{Theorem} Unfortunately, for our divide and conquer algorithm, $T(n) = \Omega(n^2)$.

\textbf{Proof (by strong induction)} Base: $H(1)$: $1 = T(1) \geq 1^2 = 1$

Then by strong induction (which we use because we have an $n / 2$ term in our formula) $H(k / 2)$ holds, so $T(k / 2) \geq \left(\frac{k}{2}\right)^2 = \frac{k^2}{4}$

$T(k) = 4T\left(\frac{k}{2}\right) + Ck \geq 4 \left(\frac{k^2}{4}\right) + Ck = k^2 + Ck \geq k^2$

\underline{Karatsuba's Algorithm}

$ac10^n + (ad + bc)10^{n / 2} + bd$

Key identity: $(b - a)(c - d) = bc - bd - ac + ad$

$(b - a)(c - d) + bd + ac = bc + ad$

Only three $n / 2$ multiplications (plus some adds and shifts!)

1. ac
2. bd
3. (b - a)(c - d)

CODE:

Karatsuba($x, y, n$):
    if $n = 1$:
        return $x \cdot y$                  // base case

    Let $m := ceil(n / 2)$                  // split
    Write $x = 10^ma + b$
    Write $y = 10^mc + d$

    Let $e := Karatsuba(a, c, m)$           // recurse
    Let $f := Karatsuba(b, d, m)$
    Let $g := Karatsuba(b - a, c - d, m)$

    Return $10^{2m}e + 10^m (e + f + g) + f$ // merge

\textbf{Claim:} Karatsuba is correct

$H(n)$: Karatsuba$(x, y, n)$ is correct.

Induct on $n$, the number of digits

Base case $H(1)$: trivial (included in the algorithm!)

Then by strong induction, $\frac{k}{2} < k$ and $H\left(\frac{k}{2}\right)$ holds

so $e = ac$, $f = bd$, and $g = (b - a)(c -d)$ by inductive hypothesis

So Karatsuba($x, y, n$) produces $ac10^n + (ad + bc)10^{n / 2} + bd$ as desired.

\textbf{Claim:} $T(n) = 3T\left(\frac{n}{2}\right) + Cn$

Why? $3T\left(\frac{n}{2}\right)$ involves 3 recursive calls to Karatsuba, and the return statement takes $O(n)$ time (because there are two shifts)

By recursion trees:

$\sum_{i = 0}^{\log_2n} cn\left(\frac{3}{n}\right)^i$

Using an identity for geometric sum, $S = \Theta(1)$ for $r < 1$ and $S = \Theta(r^l)$ when $r > 1$

$r^l = \left(\frac{3}{2}\right)^{\log_2n}$

So the runtime of Karatsuba's algorithm is $O(n^{\log_23})$

Leveraging the Fast Fourier Transform, we can multiply in $O(n \log n)$ time

Solving recurrences: \textbf{The Master Theorem}

Generic divide-and-conquer algorithm: split into $a$ pieces of size $\frac{n}{b}$ and merge/combine in time $O(n^d)$

Recipe for recurrences of the form $T(n) = a \cdot T(n / b) + Cn^d$

Mergesort: $a = 2$, $b = 2$, $d = 1$

Karatsuba: $a = 3$, $b = 2$, $d = 1$

Recursion tree $T(n) = aT(n / b) + Cn^d$

At level $i$ of the recursion tree, we'll have $a^i$ pieces of size $n / b^i$, and our work will be $a^i c\left(\frac{n}{b^i}\right)^d$

The last level in the tree will be $log_bn$, so our total work will be $\sum_{i = 0}^{log_bn} a^i c \left(\frac{n}{b^i}\right)^d$

So we have three cases: \begin{itemize}
    \item $\left(\frac{a}{b^d}\right) > 1$: then we have $Cn^d \left(\frac{a}{b^d}\right)^{log_b n} = Cn^d \frac{a^{\log_bn}}{n^d} = Cn^{\log_b a} = \Theta(n^{\log_b n})$
    \item $\left(\frac{a}{b^d}\right) = 1$: then we have $Cn^d(\log_bn + 1) = \Theta(n^d \log_bn)$
    \item $\left(\frac{a}{b^d}\right) < 1$: then $cn^dc' = \Theta(n^d)$
\end{itemize}

\underline{Lecture 7 (Selection and Medians)}

\underline{Divide and Conquer: Binary Search}

\begin{itemize}
    \item Search sorted array in time $O(\log n)$
    \item Divide and conquer approach: find the middle of the list, recursively search half of the list
    \item Item: eliminates half the list each time
    \item Proof of correctness: induction
    \item Analyze running time by Master Theorem: $T(n) = T(n / 2) + C$
\end{itemize}

Selection: Median

Given an array of numbers $A[1, \ldots, n]$, how quickly can we find the: \begin{itemize}
    \item Smallest number? $O(n)$
    \item Second smallest number? $O(n)$
    \item $k^th$ smallest? $O(kn)$
    \item Median? $O(n^2)$
\end{itemize} We can do better, and select the $k$-th smallest in $O(n)$ time

25 horses problem: find 3 fastest horses in only 7 races

\textbf{Median of Medians}
Finding median in list of length 5: $O(1)$
Number of times we find the median: $O(n) = n / 5$
# of operations, including recursive call: $n / 5 \cdot C = O(n)$
Median of medians running time: $Cn + $ time to run select (find median) on list of length $n$

Claim: for every A, there exists at least $\frac{3n}{10}$ items that are smaller than $MOM(A)$.


7/19/2021

Dynamic Programming

Dynamic programming is careful and smarter recursion \begin{enumerate}
    \item Break the problem up into small pieces and recursively solve it (like divide and conquer)
    \item Reuse solutions as necessary when sub-problems repeat
    \item Don't combine solutions like in divide and conquer
    \item Often the only polynomial time algorithm
\end{enumerate}


Lecture 9

Weighted interval scheduling

How can we optimally schedule a resource? (A classroom, for example)

Input: $n$ intervals $(s_i, f_i)$ each with value $v_i$

Assume intervals are sorted so $f_1 < f_2 < \cdots < f_n$

Output: a compatible schedule S maximizing the total value of all intervals \begin{itemize}
    \item A schedule is a subset of intervals $S \subseteq \{ 1, \ldots, n \}$
    \item A schedule $S$ is compatible if no $i, j \in S$ overlap
    \item The total value of $S$ is $\Sum_{i \in S} v_i$
\end{itemize}

Subproblems: Let $O_i$ be the optimal schedule using only the intervals $\{ 1, \ldots, i \}$

Case 1: Final interval is not in $O_i$ $(i \notin O_i)$ \begin{itemize}
    \item Then $O_i$ mustbe the optimal solution for $\{ 1, \ldots, i - 1 \}$
    \item $O_i = O_{i - 1}$
\end{itemize}

Case 2: Final interval is in $O_i$ $(i \in O_i)$ \begin{itemize}
    \item Assume intervals are sorted so that $f_1 < f_2 < \cdots < f_n $
    \item Let $p(i)$ be the largest $j$ such that $f_j < s_i$
    \item Then $O_i$ must be $i +$ the optimal solution for $\{ 1, \ldots, p(i) \}$
    \item $O_i = \{ i \} + O_{p(i)}$
\end{itemize}

Relevant subproblems: $O_i$, $O_{p(i)}$

Let $OPT(i) = \max{OPT(i - 1), v_i + OPT\left(p(i)\right)}$

$OPT(0) = 0$, $OPT(1) = v_1$

FindOpt(n):
\quad if $(n = 0)$: return 0
\quad else if $(n = 1)$: return $v_1$
\quad else: return $\max{FindOpt(n-1), v_n + FindOpt(p(n))}$

The worst-case runtime of interval scheduling is exponential.

This is \textbf{NOT} dynamic programming because none of the answers to the subproblems are being stored.

Interval scheduling: top-down

$M := [ 0, v_1 ]$
FindOPT(n):

\quad if (M[n] not empty): return $M[n]$

\quad else:

\quad \quad M[n] := max{FindOPT(n-1), v_n + FindOPT(p(n))}

\quad \quad return M[n]

What's the running time of this implementation of FindOPT(n)?

$n - 1$ elements to fill

Each fill is two recursive calls

The total time is $2(n - 1) = O(n)$

Bottom up is an iterative approach, top-down is iterative.

Interval scheduling: bottom up

FindOpt(n):

\quad M[0] = 0

\quad M[1] = v_1

\quad for (i from 2 to n):

\quad \quad M[i] = max{M[i - 1], v_i + M[p(i)]}

\quad return M[n]

What's the runtime of this bottom-up implementation? $O(n)$

\end{document}