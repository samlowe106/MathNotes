\input{../preamble.tex}

\title{Multivariable and Vector Calculus Notes}
\author{Sam Lowe}

\begin{document}

\maketitle

\tableofcontents

\pagebreak

\addcontentsline{toc}{section}{Multivariable Functions}
\begin{center}
\section{Scalar and Vector-Valued Multivariable Functions}
\end{center}

An open set is

A closed set is

Notice that sets can be simultaneously open and closed!

A \textbf{real-valued multivariable function} is a function $f : \R^n \to \R$, taking two or more independent variables and producing one real number. Because the terminology of linear algebra is so convenient for describing multivariable functions and space, we also call functions that output only a single value \textbf{scalar-valued functions}. Continuous multivariable functions $f: \R^n \to \R$ define a surface in $\R^{n+1}$.

For example, the function $z = f(x,y)$ is a function from $\R^2$ to $\R$ takes in two variables $x$ and $y$ from $\R^2$ and outputting a third variable $z$; this information can be bundled together and represented by the vector $$\begin{bmatrix}
x\\
y\\
z
\end{bmatrix} $$ which points from the origin to a point on the surface generated by $f$ in $\R^3$.

The values of $x$ and $y$ determine the position of our point with respect to the horizontal $xy$-plane, and the value $z=f(x,y)$ determines how far above that plane our point is (when $z$ is positive) or how far below the plane our point is (when $z$ is negative), or if the point is on the $xy$-plane (which only happens when $z=0$).

Unfortunately, simple analogies for the one above are difficult to provide for functions that create surfaces in $\R^4$, $\R^5$, and beyond, but many of the same geometric relations and computations still apply.

\subsection{Vector-Valued Functions}

A function $f: \R^n \to \R$ is a scalar-valued function of $n$ variables and forms a surface in $\R^{n+1}$. A function $\vec{r}: \R \to \R^n$ is a \textbf{vector-valued function} (sometimes just called a \textbf{vector function})and traces a line through $\R^n$. $\vec{r}$ can be considered a vector starting at the origin and tracing a curve $C$ through space, with the endpoint of that vector at $t$ being $\vec{r}(t)$. In the three-dimensional case, these vector-valued functions usually take the form $$r(t) = f(t)\hat{i} + g(t)\hat{j} + h(t)\hat{k} $$ Where $f$, $g$, and $h$ are functions from $\R \to \R$. In general, whenever a vector-valued functions component functions (in this case $f$, $g$, and $h$) are differentiable, then we define $$\vec{r}'(t) = \frac{d\vec{r}}{dt} = \lim_{\Delta t \to 0} \frac{\vec{r}(t + \Delta t) - \vec{r}(t)}{\Delta t} = f'(t)\hat{i} + g'(t)\hat{j} + h'(t)\hat{k}$$ $\vec{r}'(t)$ generates the \textbf{tangent vector} to $\vec{r}$ at $t$. If $\vec{r}'(t)$ exists at a point $a$, then we say that the curve $C$ generated by $\vec{r}$ is \textbf{smooth at} $\mathbf{a}$. If $C$ is smooth at every point on the domain of $\vec{r}$ (or equivalently, if $\vec{r}'(t)$ exists at every point on $\vec{r}$) then we say that $C$ is \textbf{smooth}.

\section{Limits of Multivariable Functions}

Let $U \subseteq \R^n$ be an open set on which a real-valued function $f$ is defined, and let $a = (a_1, \ldots, a_n) \in U$. We say that \textbf{the limit of $\mathbf{f(x,y)}$ as $\mathbf{(x_1,\ldots, x_n)}$ approaches $\mathbf{a}$} is $L$ and write $$\lim_{(x_1,\ldots, x_n) \to a} f(x_1,\ldots, x_n) = L$$ if for every $\epsilon > 0$ there is some $\delta > 0$ such that $(x_1,\ldots, x_n) \in U$ and $0 < \sqrt{(x_1-a_1)^2 + \cdots + (x_n-a_n)^2} < \delta$ implies $|f(x_1, \ldots, x_n)-L| < \epsilon$.

\subsection{Continuity}

We say $f$ is \textbf{continuous at} $\mathbf{a} \in \dom{f}$ if $$\lim_{(x_1, \ldots, x_n)\to a} f(x_1, \ldots, x_n)=f(a)$$ If this limit does not exist, then we say $f$ is \textbf{discontinuous at} $\mathbf{a}$. We say $f$ is \textbf{continuous on} $\mathbf{U}$ if $f$ is continuous at every point on the open set $U$; if $f$ is discontinuous at every point on $U$, we say that $f$ is \textbf{discontinuous on} $\mathbf{U}$.

\section{Differential Multivariable Calculus}

\subsection{Directional Derivatives}

Let $x_0 \in E \subseteq \R^n$, $f : E \to \R$, $\vec{v}$ a vector in $\R^n$, and $h > 0$ a real number. We define the \textbf{directional derivative of $f$ in the direction $\vec{v}$ at $a$} to be $$D_{\vec{v}}f(a) = \lim_{h \to 0} \frac{f(a + h\vec{v}) - f(a)}{h}$$ if this limit exists. Given an $n$-dimensional vector $\vec{v} = (v_1, v_2, \ldots, v_n)$, then $$D_{\vec{v}}f = \sum_{i = 1}^n v_i\frac{\partial f}{\partial x_i} $$ where each $\frac{\partial f}{\partial x_i}$ is the directional derivative of $f$ with respect to the $i^{th}$ basis vector of $\R^n$. These specific cases of the directional derivative are called \textbf{partial derivatives}.

\subsection{Partial Derivatives}

The \textbf{partial derivative of $f : \R^n \to \R$ at a point $a \in \R^n$ with respect to $x_i$} is defined as the derivative in the direction of the $i^{th}$ basis vector of $\R^n$, and is denoted $$\frac{\partial}{\partial x_i} f(a) = f_{x_i}(a) = D_{x_i}f(a)$$ By the definition of the directional derivative, $$ \frac{\partial }{\partial x_i} f(a) = \lim_{h \to 0} \frac{f(a_1, \ldots, \, a_i + h, \ldots, a_n) - f(a_1, \ldots, a_i, \ldots, a_n)}{h}$$ Because the $i^{th}$ variable is the only one changing, we can take the derivative by holding the other dependent variables constant; this neatly \uline{reduces calculating a partial derivative to an exercise in single-variable calculus,} and all of the properties of $\frac{\partial}{\partial x_i}$ and $D_{\vec{v}}$

\subsection{The Gradient}

Let $f: \R^n \to \R$ be continuously-differentiable. We define the \textbf{gradient} of $f$, denoted $\nabla f$, to be the \uline{vector} consisting of $f$'s partial derivatives with respect to each independent variable: $$ \nabla f =
\begin{bmatrix}
f_{x_1}\\
\vdots\\
f_{x_n}\\
\end{bmatrix}$$ $\nabla$ (pronounced ``nabla'' or ``del'') is the operator that sends $f$ to its gradient.

\subsubsection{Properties of the Gradient} The gradient operator has several properties: \begin{itemize}
\item If $\nabla f = \vec{0}$, then $D_{\hat{v}}f = 0$ for any $\hat{v}$.
\item $D_{\vec{v}}f$ has its max value of $||\nabla f(x,y)||$ and this will happen when $\vec{v}=c \cdot \nabla f$ ($c$ is a scalar—so $\vec{v}$ and $\nabla f$ are in the same direction). Proof: $D_{\vec{v}}f = \nabla f \cdot \vec{v} = || \nabla f || \cdot || \vec{v} || \cdot \cos{\theta} = || \nabla f || \cdot \cos{\theta}$, so when the angle between $\nabla f$ and $\vec{v}$ is zero, $\cos{\theta}$ attains its maximum value and the vectors are parallel. This shows that $\nabla f$ is the direction of steepest ascent.
\item The direction of steepest descent at a point is $-||\nabla f||$ (when $\theta = \pi$). The proof is similar to the one above.
\end{itemize}

\subsection{Higher-Order Partial Derivatives}

The partial derivatives of a function $f: \R^n \to \R$ with respect to $x_i$, if it exists, is also a function $f_{x_i}: \R^n \to \R$ and can be partially differentiated again with respect to any of the variables that $f$ takes. When a function is differentiated with respect to the same variable—for example, the independent variable $x$—$n$ times, we call this a \textbf{higher-} or $\mathbf{n^{th}}$\textbf{-order partial derivative of} $\mathbf{f}$ \textbf{with respect to} $\mathbf{x}$ and write\footnote{Liebniz's notation of $\frac{\partial^{n}}{\partial x^n} f$  is easier to write as $n$ increases; $f_{xxx \ldots}$ gets out of hand quickly.} it as $$\frac{\partial^n}{\partial x^n} f$$

\subsubsection{Mixed Partial Derivatives}

A higher-order derivative with respect to more than one variable is called a \textbf{mixed partial derivative}. For example, the partial derivative $f_x$ with respect to the independent variable $y$, we write $$(f_x)_y = f_{xy} = D_{xy} f = \frac{\partial}{\partial y} \Big[ \frac{\partial}{\partial x} f \Big] = \frac{\partial^{2}}{\partial y \partial x} f$$ The order of the variables matters. \textit{Notice the difference in the order of variables between the two notations!}

\subsubsection{Clairaut's Theorem}

Recall that a function is said to be continuously differentiable if its derivative is continuous.

\Thrm{Clairaut} If $f$ is twice continuously differentiable, then $$\frac{\partial}{\partial x_j}\frac{\partial}{\partial x_i} f(a) = \frac{\partial}{\partial x_i}\frac{\partial}{\partial x_j} f(a)$$

\subsection{The Laplacian}

The \textbf{Laplace operator} or \textbf{Laplacian}, denoted $\nabla^2 f$, $\nabla \cdot \nabla$, or $\Delta$,\footnote{Not to be confused with the usage of $\Delta$ to represent a finite change in quantity.} is the sum of the second derivatives of a function $f: \R^n \to \R$: $$\nabla^2 f = \sum_{i = 1}^n \frac{\partial^2 f}{\partial x_i}$$

\subsubsection{A Note on Poisson's, Laplace's, and Helmholtz's Equations}

When $\nabla^2 f = h$ for some function $h: \R^n \to \R$, then $f$ satisfies \textbf{Poisson's Equation}. In the homogeneous case where $\nabla^2 f = 0$, then $f$ satisfies \textbf{Laplace's Equation}; solutions of Laplace's equation are called \textit{harmonic functions}. The general theory of solutions to Laplace's equation is called \textit{potential theory}.

Laplace's and Poisson's equations are the simplest examples of elliptic partial differential equations. The eigenvalue problem for the Laplace operator corresponds to the linear partial differential equation $\nabla^2 f = -k^2 f$ (where $k^2$ is the eigenvalue and $f$ is the eigenfunction), known as the \textbf{Helmholtz equation}.

\subsection{Differentials}

\subsection{Differentials of Multivariable Functions}
Words \pagebreak

\subsection{Tangent Planes}

\subsubsection{Computing Tangent Planes}

To find a tangent plane, we need a point $p = (x_0, y_{0}, z_{0})$ and a normal vector $\vec{n} = a\hat{i} + b\hat{j} + c\hat{k}$. Then, the equation for the plane at point $p$ with normal vector $\vec{n}$ is $a(x - x_{0}) + b(y - y_{0}) + c(z - z_{0})=0$.

To find the normal line to that plane,

$$\frac{x - x_0}{a} = \frac{y - y_0}{b} = \frac{z - z_0}{c}$$

$$x = x_0 + at, \quad y = y_0 + bt, \quad z = z_0 + ct$$


\subsection{Maxima, Minima, Manifolds, and Lagrange Multipliers}

Let $U \subseteq \R^n$. A function $f : U \to \R$ has a local minimum (or local maximum) on $D$ at $p \in D$ iff there exists an open ball $B$ centered at $p$ such that $f(x) \leq f(p)$ (or $f(x) \geq f(p)$, respectively) for all points $x \in B \cap D$.

An $n$-dimensional surface (or $n$-dimensional manifold) is a set in $\R^{n+1}$ which at each point has an $n$-dimensional tangent plane.

The set $M \in \R^{n+1}$ is said to have a $k$-dimensional tangent plane at the point $a \in M$ if the union of all tangent lines at $a$ to differentiable curves on $M$ passing through $a$ forms a $k$-dimensional plane.

Let $\pi_i : \R^{n+1} \to \R^n$ be the projection mapping that deletes the $i^{th}$ coordinate; that is, $$\pi_i(x_1, \ldots, x_{i - 1}, x_i, x_{i+1}, \ldots, x_n) = (x_1, \ldots, x_{i - 1}, x_{i+1}, \ldots, x_n)$$

A set $P \in \R^{n+1}$ is called an $n$-dimensional patch iff for some positive integer $i \leq n + 1$, there exists a differentiable function $h : U \to \R$ on an open subset $U \subset \R^n$ such that $$P = \{ x \in \R^{n+1} : \pi_i(x) \in U \text{ and } x_i = h(\pi_i(x))\} $$

In other words, $P$ is the graph in $\R^{n+1}$ of the differentiable function $h$, regarding $h$ as defined on an open subset of the $n$-dimensional coordinate plane $\pi_i(\R^{n+1})$ spanned by the unit basis vectors $v_1, \ldots, v_{i-1}, v_{i+1}, \ldots, v_n$.

In other other words, the $i^{th}$ coordinate of a point of $P$ is a differentiable function of the its remaining $n$ coordinates.

The set $M \in \R^{n+1}$ is called an $n$-dimensional manifold (or $n$-manifold) iff each point $a \in M$ lies in an open subset $U \subseteq \R^{n+1}$ such that $U \cap M$ is an $n$-dimensional patch. Roughly speaking, a manifold is a union of patches that intersect ``correctly."

Theorem If $M$ is an $n$-manifold in $\R^P{n+1}$, then $M$ has an $n$-dimensional tangent plane at each of its points.

To determine when a set $M \in \R^{n+1}$ forms a manifold, we need to prove the Implicit Function Theorem.

\subsection{Implicit Differentiation and the Implicit Function Theorem}

Let $G : \R^{n+1} \to \R$ be continuously differentiable and suppose $G(a) = 0$ while $D_nG(a) \neq 0$. Then there exists a neighborhood $U$ of $a$ and a differentiable function $F$ defined on a neighborhood $V$ of $(a_1, \ldots, a_n) \in \R^n$ such that $$ U \cap G^{-1}(0) = \{ x \in \R^n : (x_1, \ldots, x_n) \in V \textbf{ and } x_n = F(x_1, \ldots, x_n)\}$$ In particular, $$ G(x_1, \ldots, x_n, F(x_1, \ldots, x_n)) = 0 $$ for all $(x_1, \ldots, x_n) \in V$.

Theorem Suppose that $g : \R^{n+1} \to \R$ is continuously differentiable. If $M$ is the set of all those points $x \in S^{-1} = g^{-1}(0)$ at which $\nabla g(x) \neq 0$, then $M$ is an $n$-manifold. Given $a \in M$, the gradient vector $\nabla g(a)$ is orthogonal to the plane tangent to $M$ at $a$.

Theorem (Lagrange Multipliers) Suppose $g: \R^n \to R$ is continuously differentiable, and let $M$ be the set of all points $x \in \R^n$ at which both $g(x) = 0$ and $\nabla g(x) \neq 0$. If the differentiable function $f : \R^n \to \R$ attains a local maximum or local minimum at $a \in M$, then $$\nabla f(a) = \lambda \nabla g(a) $$ for some $\lambda$, called the Lagrange multiplier.

Theorem (Implicit Mapping Theorem) Let $G : \R^n \to \R^m$, $(m < n)$, be a continuously differentiable mapping. \dots

Theorem (General Method of Lagrange Multipliers) ...




% When taking a partial derivative, keep in mind that dependent variables must be implicitly differentiated. For example, for a function $z = f(x,y)$, the variable $z$ must be implicitly differentiated (such as in the case $f(x, y) = x^2y + xz + yz^2 = 8$, we implicitly differentiate $z$).

% Even if all the partial derivatives of $f$ exist at $a$, $f$ is not necessarily continuous at $a$; for example, consider $$f(x,y) = \left\{ \begin{array}{cc}
% \displaystyle\frac{xy}{x^2+y^2} & (x, y)\neq (0,0) \\
% 0 & \text{otherwise.}
% \end{array}\right.
% $$ $f_x(0,0) = f_y(0,0) = 0$, but $f(x,y)$ is undefined and therefore discontinuous at $(0,0)$.

% However, if all partial derivatives of $f$ exist in the neighborhood around $a$ and are continuous there, then $f$ is totally differentiable (a $C^1$ function) at $a$.

\section{Local Maxima, Local Minima, and Classification of Critical Points}

The maxima and minima of a function $f: \R^n \to \R$ exist at the \textbf{critical points} of $f$. Not all critical points of $f$ are maxima or minima, however; if a critical point is neither a local minima nor a local maxima, it is called a \textbf{saddle point}. $f$ has its critical points wherever $\nabla f$ is zero or undefined.

We can use the \textbf{Hessian matrix} (or \textbf{Hessian} for short) of $f$ to determine if a given point $p$ is a maximum or minimum of $f$. The Hessian of $f: \R^n \to \R$ is a matrix of its second derivatives, where the entry in the $i^{th}$ row and $j^{th}$ column is $f_{x_i}{x_j}$: $$\begin{bmatrix}
f_{x_1x_1} & f_{x_1x_2} & \cdots & f_{x_1x_n}\\
f_{x_2x_1} & f_{x_2x_2} & \cdots & f_{x_2x_n}\\
\vdots     & \vdots     & \ddots & \vdots\\
f_{x_nx_1} & f_{x_nx_2} & \cdots & f_{x_nx_n}\\
\end{bmatrix}$$ Because all partial derivatives of $f$ must exist, $f_{x_ix_j} = f_{x_jx_i}$ by Clairaut's theorem so \uline{the Hessian is symmetric.}

\subsection{Two-Variable Case}

If $f(x,y)$ is a function of two variables, then the determinant of the Hessian of $f$ is defined as $$D = \begin{vmatrix}
f_{xx} & f_{xy} \\
f_{yx} & f_{yy}
\end{vmatrix}
= f_{xx}(a,b)f_{yy}(a,b) - \big( f_{xy}(a,b) \big)^{2}$$ and the second derivative test is as follows: \begin{itemize}
\item If $D > 0$ and $f_{xx}(a,b) > 0$, then $f(a,b)$ is a local minimum.

\item If $D > 0$ and $f_{xx}(a,b) < 0$, then $f(a,b)$ is a local maximum.

\item If $D < 0$, then $f(a,b)$ is a saddle point.

\item If $D = 0$, the test is inconclusive.
\end{itemize}

\subsection{General Case}

For functions of three or more variables at a critical point $p$, the determinant doesn't provide enough information for a conclusive test, so a different test must be applied: \begin{itemize}
\item If the Hessian of $f(p)$ is positive-definite (all of its eigenvalues are positive), then $f(p)$ is a local maximum.

\item If the Hessian of $f(p)$ is negative-definite (all of its eigenvalues are negative), then $f(p)$ is a local minimum.

\item If the Hessian has both positive and negative eigenvalues, then $f(p)$ is a saddle point of $f$.
\end{itemize} In any other case, the test is inconclusive.

\section{Lagrange Multipliers}

Consider trying to maximize or minimize a function $f(x,y,z)$ with the constraint that $g(x,y,z) = k$. This problem can be interpreted as trying to maximize or minimize $f(x,y,z)$ while $(x,y,z)$ is restricted to the level curve $g(x,y,z)=k$.

First, find all values of $x, y, z,$ and $\lambda$ such that $$\nabla f(x,y,z) = \lambda \nabla g(x,y,z) \text{ and } g(x,y,z) = k$$ Then, evaluate $f$ at all the points $(x,y,z)$ that result from the above. The largest of these values is the maximum for $f$; the smallest is the minimum.

\begin{comment}

\section{Level Curves}

A \textbf{level curve} (or \textbf{level plane}) can be thought of as a horizontal cross-section or topographical height map. The level curve or plane is always one dimension less than the function the level curve is taken from.

The level curve at $c$ is represented by a vector function such as $\vec{r}(t) = x(t)\hat{i} + y(t)\hat{j}$, so the function for the level curve is $f(x(t)\hat{i}, y(t)\hat{j}) = c$. $\vec{r}'(t)$ gives the slope of the tangent vector of the level curve.

$f'(x(t)\hat{i}, y(t)\hat{j}) = 0$

$\displaystyle{\frac{\partial f}{\partial y} \cdot \frac{\partial y}{\partial t} + \frac{\partial f}{\partial x} \cdot \frac{\partial x}{\partial t}} = 0$

$\nabla f(x,y) \cdot \vec{r}(t)=0$

This means that $\nabla f(x,y)$ and $\vec{r}(t)$ are orthogonal, so $\nabla f$ is the normal vector to any point on a level curve.

Example: Find the tangent line and normal line to $x^{2} - y^{2} = 16$ @ $P(5, 3)$ (this function is 2-dimensional)

Let the above function be a level curve for some surface in $\mathbb{R}^{3}$. So, $f(x, y) = x^{2} + y^{2}$ (this function is 3-dimensional, and the constant $16$ can be included or excluded with no change) and $x^{2} - y^{2} = 16$ is the level curve at $16$.

$\nabla f (x, y) = \begin{bmatrix}2x\hat{i} + 2y\hat{j}\end{bmatrix}$

so $\nabla f(5, 3)$ is the normal to the level curve at $P(5, 3)$

Example: find the normal vector to $-x^{2} + y^{2} - z^{2} = 4$ @ $P(1, 3, 2)$

$f(x, y, z) = -x^{2} + y^{2} - z^{2}$. $f(x, y, z)= 4$ is the level surface that contains $(1, 3, 2)$

$\nabla f(1, 3, 2)$ = the specific normal to $-x^{2} + y^{2} - z^{2} = 4$ @ $P(1, 3, 2)$.

General method: given $f(x, y, ...) = c$, let $f(x, y, ...)$ be a function and $f(x, y, ...) = c$ be the level curve at $c$. Find $\nabla f$ and plug in the values of the given point $P$.
\end{comment}


\pagebreak
\addcontentsline{toc}{section}{Integration}
\begin{center}
\section{Integration}
\end{center}

\subsection{The Lebesgue Measure}

There are several different ways to define integration. The standard way, the Riemann integral, is somewhat tired by the time most students pursue analysis in several variables. Furthermore, there are stronger notions of integration that not only agree with the Riemann integral where it's defined, but extend our ability to integrate to a larger class of functions. The ideal candidate for this definition of integration, as well as to introduce the field of measure theory, is the \textbf{Lebesgue integral}.

First, we must outline several properties that we would expect any reasonable definition of area or volume to encompass. \begin{enumerate}
    \item If $A$ and $B$ are disjoint, then we would expect $m(A + B) = m(A) + m(B)$
    \item If $A \subseteq B$, we would expect $m(A) \leq m(B)$
    \item $m(\vec{v} + A) = m(A)$; that is, $A$'s position in space doesn't affect its volume
    \item That the unit cube $(0,1)^n$ has measure 1
\end{enumerate} Unfortunately, no measure with the above properties can apply to \textit{every} region in $\R^n$. One such counterexample is the Banach-Tarski paradox, in which a unit ball in $\R^3$ is decomposed into five pieces, which are then reassembled into two disjoint unit spheres -- completely violating the concept of conservation of volume.

However, a non-negligible class of sets in $\R^n$ can be measured this way -- these sets are called the \textbf{measurable sets}. \uline{All of the above properties hold when we restrict our attention to these sets,} and \uline{these sets turn out to contain most of the ``useful'' sets that we are typically concerned with in the real world} -- for example, \uline{all sets that are open or closed are measurable.} With this in mind, we now define the properties to which measurable sets adhere: \begin{enumerate}
    \item
\end{enumerate}

We now define the Lebesgue measure on measurable sets, which obeys the following properties: \begin{enumerate}
    \item $m(\emptyset) = 0$
    \item $0 \leq m(\Omega) \leq \infty$
    \item $A \subseteq B$, then $m(A) \leq m(B)$
    \item
\end{enumerate}

Theorem (Existence of the Lebesgue Measure)

\Def{Measureable Set} We define a set $E \subseteq \R^n$ to be \textbf{Lebesgue measurable} (or simply \textbf{measurable}) if it obeys the following identity: $$m^*(A) = m^*(A \cap E) + m^*(A \backslash E)$$ for every subset $A \subseteq \R^n$.

Essentially, this means that if we use $E$ to split some arbitrary set $A$ into two parts, we retain additivity. One can think of measurable sets as ones where finite additivity holds.

\Thrm{Lebesgue Integral Generalize Riemann Integral} Let $I \subseteq \R$ be an interval, and let $f : I \to \R$ be a Riemann-integrable function. Then $f$ is also absolutely (Lebesgue) integrable, and the Riemann and Lebesgue integrals agree.

The above theorem provides a bridge between the Lebesgue integral, with all of its desirable properties, and the Riemann integral, which we can assign concrete values to.

It should be noted that the converse of the above theorem is not true -- there are functions that are Lebesgue integrable but not Riemann integrable, most notably the Dirichlet function. Additionally, the Lebesgue integral also behaves well with limits -- see the Lebesgue monotone convergence theorem, Fatou's Lemma, and the Lebesgue dominated convergence theorem.

\subsection{Multiple Integrals}

\begin{comment}
Recall that $$\int_{a}^{b} f(x) dx = \lim_{n \to \infty} \sum_{i=1}^{n} f(x_{i}^{*})\Delta x $$

We can similarly define an integral of a function $f(x,y)$ defined on a closed rectangle $R = [a,b] \times [c,d] = \big\{ (x,y) \in \mathbb{R}^{2} \, | \, a \leq x \leq b, \, c \leq y \leq d \big\} $

The graph of $f$ is a surface with equation $f(x,y) = z$. Let $S$ be the solid that lies above $R$ and under the graph of $f$ such that $$S = \{(x,y,z) \in \mathbb{R}^{3} \, | \, 0 \leq z \leq f(x,y), \, f(x,y) \in \mathbb{R}^{2}\}$$

Likewise, the double integral over the rectangle $R$ is defined $$\iint\limits_{R} f(x,y) dA = \lim_{n,m \to \infty} \sum_{i=1}^{m} \sum_{j=1}^{n} f(x_{ij}^{*}, y_{ij}^{*})\Delta A$$

More precisely, for every number $\epsilon > 0$ there is an integer $N$ such that

$$\Big| \iint\limits_{R} f(x,y) dA - \lim_{n,m \to \infty} \sum_{i=1}^{m} \sum_{j=1}^{n} f(x_{ij}^{*}, y_{ij}^{*})\Delta A \Big| < \epsilon$$

for all integers $n$ and $m$ greater than $N$ and for any choice of sample points $(x_{ij}^{*}, y_{ij}^{*}) \in R_{ij}$. A function $f$ is called integrable if such a limit exists; all continuous functions are integrable.
\end{comment}

\subsection{Properties of Double Integrals}

\subsubsection{Fubini's Theorem}



\Thrm{Fubini} Let $A \subseteq \R^m$ and $B \subseteq \R^n$ and let $f : \R^{m + n} = \R^m \times \R^n \to \R$ be an absolutely integrable function. Then there exist such functions $f_x : \R^m\to \R$ and $f_y : \R^n \to \R$ such that, $f(x,y)$ is absolutely integrable with $$f_x(y) = \int_A f(x,y) dx$$ and $$f_y(x) = \int_B f(x,y) dy$$ then \todo clean up! $$ \int_B \left( \int_A f(x,y) dy \right) dx = \int\limits_{A \times B} f = \int_A \left( \int_B f(x,y) dy \right) dx$$ By repeatedly applying this theorem, \uline{we can split one $n$-dimensional integral into $n$ one-dimensional integrals}, and \uline{we can change the order in which we integrate the variables.}

\Thrm{Fubini's Theorem for Products} In the special case where $f(x,y)$ can be factored as the product of a function $g(x)$ and a function $h(y)$ only, then $$\iint\limits_{A \times B} f(x,y) \, dA = \left(\int_a^b g(x) \, dx \right) \left(\int_c^d h(y) \, dy \right)$$





Assume that all of the following integrals exist.

$$\iint\limits_{D} f(x,y) + g(x,y) \, dA = \iint\limits_{D} f(x,y) \, dA + \iint\limits_{D} g(x,y) \, dA$$

$$\iint\limits_{D} cf(x,y) \, dA = c\iint\limits_{D} f(x,y) \, dA \text{ for constant } c $$

if $f(x,y) \geq g(x,y)$ for all $(x,y) \in D$, then $$ \iint\limits_{D} f(x,y) \, dA \geq \iint\limits_{D} g(x,y) \, dA $$

If $D = D_{1} \cup D_{2}$ and $D_{1}$ and $D_{2}$ don't overlap (except perhaps on their boundaries) then $$ \iint\limits_{D} f(x,y) \, dA = \iint\limits_{D_{1}} f(x,y) \, dA + \iint\limits_{D_{2}} f(x,y) \, dA $$

The next property, which states $$ \iint\limits_{D} 1 \, dA = A(D) $$

states that when integrating the constant function $f(x,y) = 1$ over $D$, then we get the area of $D$. (This is true because any solid with base with area $D$ and whose height is $1$ will then have a volume of $A(D) \times 1 = A(D)$.)

If $m \leq f(x,y) \leq M$ for all $(x,y) \in D$ then $$ mA(D) \leq \iint\limits_{D} f(x,y) \, dA \leq MA(D) $$

\subsection{Arc Length of a Vector Function}

The arc length $s$ of a vector function $r(t)$ in $\R^n$ is $$\int_{a}^{b} \sqrt{\Big(\frac{dx_1}{dt}\Big)^{2} + \cdots + \Big(\frac{dx_n}{dt}\Big)^{2}} \, dt = \int_{a}^{b} |r'(t)|\, dt $$ We can define the arc length function as $$s(t) = \int_{a}^{t} |r'(u)| du $$ which gives the arc length of $r$ from $r(a)$ to $r(t)$. We then have $\dfrac{ds}{dt}=|r'(t)|$

\subsubsection{Reparamaterization with Respect to Arc Length}

 Reparameterization seeks to define $t$ as a function of $s$ $\big(t = t(s)\big)$ so the function can be parameterized in terms of its arc length $\big(r = r(t(s))\big)$. This can be done for solving for $t$ as a function of $s$ in the arc length formula, then plugging $t(s)$ in to $r$.


\subsubsection{Curvature}

A parameterization $r(t)$ is called \textbf{smooth} on an interval $I$ if $r'$ is continuous and $r'(t) \neq 0$ on $I$. A curve is called smooth if it has a smooth parameterization.

Recall that if $C$ is a smooth curve given by the vector function $r$, the unit tangent vector $T(t)$ is given by $$T(t) = \frac{r'(t)}{|r'(t)|} $$

The curvature of a curve is given by $$\kappa = \Big|\frac{dT}{ds}\Big| $$ alternatively $$\kappa (t) = \frac{|r'(t) \times r''(t)|}{|r'(t)|} $$



\subsection{Double Integrals over a General Region}

Suppose that $D$ is a bounded region, which means that $D$ can be enclosed in a rectangular region $R$. The we define a new function $F$ with domain $R$ such that

$$ F(x) =
\begin{cases}
f(x,y) & \text{if } x \in D\\
0      & \text{else}
\end{cases}
$$

If $F$ is integrable over $R$, then we define the \textbf{double integral of $f$ over $D$} by $$\iint\limits_{D} f(x,y) \, dA = \iint\limits_{R} F(x,y) \, dA$$

A plane region $D$ is said to be of \textbf{type I} if it lies between the graphs of two functions $g_{1}(x)$ and $g_{2}(x)$ that are continuous on $D$; that is, $$D = \big\{ (x,y) \, | \, a \leq x \leq b, \, g_{1}(x) \leq y \leq g_{2}(x) \big\}$$

in which case $$\iint\limits_{D} f(x,y) dA = \int_{a}^{b} \int_{c}^{d} F(x, y) \, dy \, dx = \int_{a}^{b} \int_{g_{1}(x)}^{g_{2}(x)} f(x, y) \, dy \, dx$$

A plane region of \textbf{type II} can be expressed as $$D = \big\{ (x,y) \, | \, c \leq y \leq d, \, h_{1}(y) \leq x \leq h_{2}(y) \big\}$$

for functions $h_{1}(y)$ and $h_{2}(y)$ that are continuous on $D$. Similar to the above, the double integral can then be defined as $$\iint\limits_{D} f(x,y) dA = \int_{a}^{b} \int_{h_{1}(y)}^{h_{2}(y)} f(x, y) \, dx \, dy$$

\subsection{Surface Integrals}

The area of the surface with equation $ z = f(x,y), \, x(,y) \in D $ where $f_{x}$ and $f_{y}$ are continuous is $$ A(S) = \iint\limits_{D} \sqrt{1 + \Big( \frac{\partial z}{\partial x} \Big)^{2} + \Big( \frac{\partial z}{\partial y} \Big)^{2} } \, dA$$

\section{Multiple Integrals}

The formulas for double integrals can be extended to multiple (triple, quadruple, ...) integrals.

\subsection{Change of Variables in Multiple Integrals}

Knowing that $x = r\cos{\theta}$, $y = r\sin{\theta}$, we can perform a change of variables similar to a reverse $u$-substitution: $$ \iint\limits_{R} f(x,y) dA = \iint\limits_{S} f(r\cos{\theta}, r\sin{\theta}) \, r \, dr \, d\theta  $$ where $S$ is a region in the $r\theta$-plane that corresponds to $R$ in the $xy$-plane.

\subsubsection{The Jacobian}

More generally, we consider a change of variables from $x$ and $y$ to $x'$ and $y'$ that is given by a transformation $T$\footnote{We usually assume that $T$ is a $\mathbf{C^{1}}$ \textbf{transformation}, which means that $f_{1}$ and $f_{2}$ have continuous first-order partial derivatives.}: $$\begin{bmatrix}
x'\\
y'
\end{bmatrix}
= T \Big(
\begin{bmatrix}
x\\
y
\end{bmatrix}
\Big) =
\begin{bmatrix}
f_{1}(x,y)\\
f_{2}(x,y)
\end{bmatrix}$$

The \textbf{Jacobian matrix} (or \textbf{Jacobian} for short) of a transformation $T$ can be used to \textit{locally approximate} $T$ as a linear transformation: $$\begin{vmatrix}
\dfrac{\partial}{\partial x} f_{1} & \dfrac{\partial}{\partial y} f_{1} \\
\dfrac{\partial}{\partial x} f_{2} & \dfrac{\partial}{\partial y} f_{2}
\end{vmatrix}
= \dfrac{\partial f_{1}}{\partial x} \dfrac{\partial f_{2}}{\partial y}
- \dfrac{\partial f_{1}}{\partial y} \dfrac{\partial f_{2}}{\partial x}
= \dfrac{\partial(x', y')}{\partial(x, y)}$$

Suppose that $T$ is a $C^{1}$ transformation with a nonzero Jacobian, and that $T$ maps $S$ in the $xy$-plane onto a region $R$ in the $x'y'$-plane. Suppose that $f$ is continuous on $R$ and that $R$ and $S$ are type I or type II planar regions. Suppose also that $T$ is one-to-one (except perhaps on the boundary of $S$). Then

$$ \iint\limits_{R} f(x,y) dA = \iint\limits_{S} f(x', y') \Big| \dfrac{\partial(x', y')}{\partial(x, y)} \Big| dx \, dy$$

\pagebreak

\addcontentsline{toc}{section}{Vector Fields}
\begin{center}
\section{Vector Fields}
\end{center}

Let $D$ be a subset of $\mathbb{R}^{n}$. A vector field over $R^{n}$ is a function that assigns an $n$-dimensional vector to every point in $D$.

Note that for a given point $p$ on a curve $C$ in a vector field given by $f$, the vector tangent to $C$ at $p$ does not necessarily equal $f(p)$ ($C$ can "go against" the vectors in the vector field).

\subsection{Line Integrals}

If $f$ is defined on a smooth curve $C$ given by $r(t) = x(t)\hat{i} + y(t)\hat{j}$ (or equivalently $x = x(t)$ and $y = y(t)$) for $a \leq t \leq b$, then the \textbf{line integral of $\mathbf{f}$ along $\mathbf{C}$} is $$\lim_{n\to\infty} \sum_{i=1}^{\infty} f(x_i^*, y_i^*) \Delta s_i = \int\limits_C f(x,y) ds$$ If $f$ is continuous, the limit exists and is equal to $$ \int_C f(x,y) ds = \int_{a}^{b} f(x(t),y(t)) \sqrt{\Big( \frac{dx}{dt} \Big)^{2} + \Big( \frac{dy}{dt} \Big)^{2} } \, dt$$ If $s(t)$ is the length of $C$ between $r(a)$ and $r(t)$, then $$\begin{array}{cl}
    \frac{ds}{dt} =&\sqrt{\Big( \frac{dx}{dt} \Big)^{2} + \Big( \frac{dy}{dt} \Big)^{2} }\\
    ds =&\sqrt{\Big( \frac{dx}{dt} \Big)^{2} + \Big( \frac{dy}{dt} \Big)^{2} } \, dt
\end{array}$$ Line integrals with respect to $x$ and $y$ can also be evaluated by expressing everything in terms of $t$. If $x(t) = t$ and $y(t) = t$, then $dx = x'(t)dt$, $dy = y'(t)dt$, and $$\int\limits_{C} f(x,y) dx = \int_{a}^{b} f(x(t),y(t)) x'(t) dt $$ $$\int\limits_{C} f(x,y) dy = \int_{a}^{b} f(x(t),y(t)) y'(t) dt $$

\subsection{Line Integrals in Space}

Now suppose $C$ is a smooth curve given by $r(t) = x(t)\hat{i} + y(t)\hat{j} + z(t)\hat{k}$ (or equivalently $x = x(t)$, $y = y(t)$, and $z = z(t)$) for $a \leq t \leq b$. If $f$ is a function of three variables that is continuous on some region containing $C$, then we define the \textbf{line integral of \textit{f} along \textit{C} with respect to arc length} as $$\begin{aligned}
    \int\limits_C f(x,y,z) ds&= \int_a^b f(x(t), y(t), z(t)) \sqrt{\Big( \frac{dx}{dt} \Big)^2 + \Big( \frac{dy}{dt} \Big)^2 + \Big( \frac{dz}{dt} \Big)^2 } \, dt\\
    &=\int_a^b f \big(r(t)\big) \, | r'(t) | \, dt
\end{aligned}$$

\subsection{Line Integrals in Vector Fields}

% At a given point $p$, the work done by $f$ on a particle travelling along $C$ is equal to the dot product of $f(c) \cdot d\vec{r}$ (where $d\vec{r}$ is the displacement of the particle on $C$). By integrating this dot product, we sum up these dot products at every point.

Recall that the work done by a variable force $f(x)$ on a particle moving from $a$ to $b$ along the $x$-axis is $\int_a^b f(x) dx$. We also know that the work done by a constant force $F$ in moving an object from a point $P$ to another point $Q$ in space is $W = F \cdot D$, where $D = \overline{PQ}$ is the displacement vector.

Now suppose that $F = P \hat{i} + Q \hat{j} + R \hat{k}$ is a continuous vector field on $\R^3$ (maybe representing the force of gravity or an electrical field, for example) and that $T(x,y,z)$ is the unit tangent vector at the point $(x,y,z)$ on $C$. The work done by $F$ on a particle moving in $\R^3$ along a smooth curve $C$ given by the vector equation $r(t) = x(t)\hat{i} + y(t)\hat{j} + z(t)\hat{k}$ is \textbf{the line integral of $F$ along $C$,} given by $$ \int\limits_{C} F \cdot dr = \int\limits_{C} F \cdot T \, ds = \int_{a}^{b} F\big(r(t)\big) \cdot r'(t) dt$$ for $a \leq t \leq b$. Keep in mind that $F(r(t))$ is an abbreviation for the vector field $F(x(t), y(t), z(t))$. We can also formally write $dr = r'(t) dt$.

Now suppose the vector field $F$ on $\R^3$ is given by $F = P\hat{i} + Q\hat{j} + R\hat{k}$. By the formula above, this integral simplifies to $$ \int\limits_{C} F \cdot dr = \int\limits_{C} P \, dx + Q \, dy + R \, dz $$

\subsection{Conservative Vector Field}

A vector field $F$ is called \textbf{conservative} if it's the gradient of some scalar function; that is, there exists a function $f$ such that $F = \nabla f$. In this case, we call $f$ a \textbf{potential function} for $F$.

\subsection{The Fundamental Theorem for Line Integrals}

Let $C$ be a smooth curve given by the vector function $r(t)$, $a \leq t \leq b$. Let $f$ be a differentiable function of several variables whose gradient vector $\nabla f$ is continuous on $C$. Then $$ \int\limits_{C} \nabla f \, dr = f\big(r(b)\big) - f\big(r(a)\big) $$ Considering $\nabla f$ as a kind of derivative of $f$, this result can be viewed as a special case of the Second Fundamental Theorem of Calculus.

\subsection{Independence of Path}

Suppose $C_{1}$ and $C_{2}$ are two piecewise-smooth curves (or \textbf{paths}) that have the same initial point $A$ and terminal point $B$. We know that in general $\int_{C_1} F \cdot dr \neq \int_{C_2} F \cdot dr $, but that $\int_{C_1} \nabla f \cdot dr = \int_{C_2} \nabla f \cdot dr $ whenever $\nabla f$ is continuous. In other words, the line integral of a \textit{conservative} vector field depends on only the initial point and terminal point of a curve.

In general, if $F$ is a continuous vector field with domain $D$, we say that the line integral $\int_C D \cdot dr$ is \textbf{independent of path} if $\int_{C_1} F \cdot dr = \int_{C_2} F \cdot dr $ for any two paths $C_1$ and $C_2$ in $D$ that have the same initial points and the same terminal points. With this terminology, we state:\linebreak

\begin{flushleft}
    \noindent\textbf{Theorem.} Line integrals of conservative vector fields are independent of path.\linebreak
\end{flushleft}

A curve is called \textit{closed} if its terminal point coincides with its initial point: $r(b) = r(a)$. If $\int_C F \cdot dr$ is independent of path in $D$, and $C$ is any closed path in $D$, we choose any two points $A$ and $B$ on $C$ and regard $C$ as being composed of the path from $C_1$ from $A$ to be $B$ followed by the path $C_2$ from $B$ to $A$. Then $$ \int\limits_C F \cdot dr = \int\limits_{C_1} F \cdot dr + \int\limits_{-C_2} F \cdot dr = \int\limits_{C_1} F \cdot dr - \int\limits_{C_2} F \cdot dr = 0 $$ since $C_1$ and $C_2$ have the same initial and terminal points.

\textbf{Theorem} $\int_C F \cdot dr$ is independent of path in $D$ if and only if $\int_C F \cdot dr = 0$ for every closed path $C$ in $D$.

For a region to be \textbf{open}, that it means that for every point $P$ in that region there is a disk centered on $P$ that also lies within the region.

For a region to be \textbf{connected}, that means that any two points in the region can be joined by a path that lies within the region.

\textbf{Theorem.} Suppose $F$ is a vector field that is continuous on an open connected region $D$. If $\int_C F \cdot dr$ is independent of path in $D$, then $F$ is a conservative vector field on $D$. (There exists a function such that $\nabla f = F$.)

If $F(x,y) = P(x,y)\hat{i} + Q(x,y)\hat{j}$ is a conservative vector field, and $P$ and $Q$ have continuous first-order partial derivatives on a domain $D$, then throught $D$ $$\frac{\partial P}{\partial y} = \frac{\partial Q}{\partial x}$$

A \textbf{simple curve} is a curve that does not intersect itself anywhere between its endpoints.

A \textbf{simply-connected region} is a connected region $D$ such that every simple closed curve in $D$ only encloses points in $D$. (A simply-connected region can't have any holes.)

The converse of the above theorem only applies in the case that $D$ is a simply connected region:

\textbf{Theorem.} Let $F = P\hat{i} + Q\hat{j}$ be a vector field on an open simply-connected region $D$. If $P$ and $Q$ have continuous first-order partial derivatives and $$\frac{\partial P}{\partial y} = \frac{\partial Q}{\partial x}$$ throught $D$, then $F$ is conservative.

\subsection{Green's Theorem}

\textbf{Green's Theorem} Let $C$ be a positively oriented, piecewise-smooth, simple closed curve in the plane and let $D$ be the region bounded by $C$. If $P$ and $Q$ have continuous partial derivatives on an open region that contains $D$, then $$\int\limits_{C} P \, dx + Q \, dx = \iint\limits_{D} \Big{(} \frac{\partial Q}{\partial x} \frac{\partial P}{\partial y}  \Big{)} dA$$

The notation $\oint_C P \, dx +  Q \, dy$ is sometimes used to indicate that a curve is calculated using the positive orientation of a closed curve $C$.

\subsection{Curl}

Recall the operator del:\footnote{del is not strictly a vector, but it is represented as a vector as a notational convenience.} $$\nabla = \frac{\partial}{\partial x}\hat{i} + \frac{\partial}{\partial y}\hat{j} + \frac{\partial}{\partial z}\hat{k}$$ If $F = P\hat{i} + Q\hat{j} + R\hat{k}$ is a vector field on $\R^3$ and the partial derivatives of $P$, $Q$, and $R$ all exist, then the \textbf{Curl} of $F$ is the vector field on $\R^3$ defined by $$ \Curl{F} = \nabla \times F =
\begin{vmatrix}
\hat{i}                     & \hat{j}                     & \hat{k} \\
\frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z} \\
P                           & Q                           & R \\
\end{vmatrix}
= \Big(\frac{\partial R}{\partial y} - \frac{\partial Q}{\partial z} \Big)\hat{i} + \Big(\frac{\partial P}{\partial z} - \frac{\partial R}{\partial x} \Big)\hat{j} + \Big(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \Big)\hat{k}
$$ Curl measures the rotation of the vector field in the neighborhood around a point, like a whirlpool. \begin{itemize}
    \item If curl is positive at a point $P$, then the fluid is spinning counter-clockwise at $P$.
    \item If curl is zero at a point $P$, then the fluid is not spinning at $P$, in which case we call $P$ \textit{irrotational}.
    \item If curl is negative at a point $P$, then the fluid is spinning clockwise at $P$.
\end{itemize} If $F$ is a conservative vector field, then $\Curl{F} = 0$. If $F$ is a vector field defined on all of $\R^3$ whose component functions gave continuous partial derivatives and $\Curl{F} = 0$, then $F$ is conservative.\newline

\textbf{Theorem.} If $f$ is a function of three variables that has continuous second-order partial derivatives, then $\Curl (\nabla f) = 0$. In other words, if $F$ is conservative, then $\Curl{F} = 0$. The converse only applies if $F$ is defined everywhere. \newline

\textbf{Theorem.} If $F$ is a vector field defined on all of $\R^3$ where component functions have continuous partial derivatives and $\Curl F = 0$, then $F$ is a conservative vector field.

\subsection{Divergence}

Let $F = P\hat{i} + Q\hat{j} + R\hat{k}$ be a vector field on $\R^3$ and $\frac{\partial P}{\partial x}$, $\frac{\partial Q}{\partial y}$, $\frac{\partial R}{\partial z}$ all exist.

Whereas the gradient of a a scalar function $F$ is a vector $\nabla F(x,y,z)$, curl is the vector field equal to $\nabla \times F$ the \textbf{divergence} of a vector field is a \textit{scalar} equal to $$\Div{F} = \nabla \cdot F = \frac{\partial P}{\partial x} + \frac{\partial Q}{\partial y} + \frac{\partial R}{\partial z}$$ Divergence is a measurement of how much fluid/flow leaves a neighborhood around a point $P$ compared to how much fluid/flow enters the neighborhood around $P$.\begin{itemize}
    \item If the amount of fluid/flow that leaves the neighborhood of $P$ is \textit{greater} than the amount that enters it, divergence is positive, in which case $P$ is said to be \textbf{divergent}. If a vector field is divergent at every point, then that vector field itself is divergent.
    \item If the amount of fluid/flow that leaves the neighborhood of $P$ is \textit{equal to} the amount that enters it, divergence is zero, in which case $P$ is said to be \textbf{incompressible}. If a vector field is incompressible at every point, then that vector field itself is incompressible.
    \item If the amount of fluid/flow that leaves the neighborhood of $P$ is \textit{less} than the amount that enters it, divergence is negative.
\end{itemize} If $F = P\hat{i} + Q\hat{j} + R\hat{k}$ is a vector field on $\R^3$ and $P$ $Q$ and $R$ have continuous second-order partial derivatives, then by Clairaut's theorem $\Div \Curl F = \nabla \cdot ( \nabla \times F) = 0$.

When we take the divergence of a gradient field $\nabla f$ for $f$ a function of three variables, we have $$\Div (\nabla f) = \nabla \cdot (\nabla f) = \nabla^2 f = \frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2} + \frac{\partial^2 f}{\partial z^2}$$

\subsection{Vector Form of Green's Theorem}

The curl and divergence operators allow us to rewrite Green's Theorem in a way that will be useful later. We suppose that the plane region $D$, its boundary curve $C$, and the function $P$ and $Q$ satisfy the hypotheses of Green's Theorem. Then we consider the vector field $F = P\hat{i} + Q\hat{j}$. Its line integral is $$\oint\limits_C F \cdot dr = \oint\limits_C P \, dx + Q \, dy $$ regarding $F$ as a vector field on $\R^3$ with third component equal to 0, we have $$\Curl{F} = \begin{vmatrix}
    \hat{i}                     & \hat{j}                     & \hat{k} \\
    \frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z} \\
    P                           & Q                           & 0 \\
\end{vmatrix} = \Big(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \Big)\hat{k}$$ Therefore $$\Curl{F} \cdot k = \Big(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \Big)\hat{k} \cdot \hat{k} = \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}$$ So we can write Green's theorem in terms of curl as $$ \oint\limits_C F \cdot dr = \iint\limits_D (\Curl{F} \cdot k) \, dA$$ We can also write Green's theorem in terms of divergence as $$\oint\limits_C F \cdot ds =  \iint\limits_D \Div{F(x,y)} \, dA$$

% Where did this statement of Green's theorem come from?
% Let $F(x,y) = P\hat{i} + Q\hat{j}$ be a vector field and let $\vec{r}(t) = x \, \hat{i} + y \, \hat{j}$ be a positively-oriented (counterclockwise) simple closed curve $C$ in that vector field that encloses a region $R$. \textbf{Green's Theorem} states that $$ \oint_C  P \, dx + Q \, dy = \iint_R \Big[ \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \Big] dA$$ Then we have $$\oint_C \big( P \, \hat{i} + Q \, \hat{j} \big) \cdot \big( dx \, \hat{i} + dy \, \hat{j} \big) = \oint_C P \, dx + Q \, dy = \oint_C F \cdot dr$$ This last integral represents the work, $\omega$, done along a simple closed curve $C$ through a vector field. In a conservative vector field, $\int_C F \cdot dr = \iint_R \Big[ \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \Big] dA = 0$ because $\frac{\partial Q}{\partial x} = \frac{\partial P}{\partial y}$. Therefore, Green's Theorem is most useful for evaluating the areas of regions bounded by simple closed curves in \textit{non-conservative} vector fields.

\section{Parametric Surfaces}

In the same way that we can describe a vector function $r(t)$ of a singe parameter $t$, we can describe a surface by a vector function $r(u,v)$ of two parameters $u$ and $v$. We suppose that $$r(u,v) = x(u,v)\hat{i} + y(u,v)\hat{j} + z(u,v)\hat{k}$$ is a vector-valued function defined on $D$ in the $uv$-plane. So $x$, $y$, and $z$, the component functions of $r$, are functions of the two variables $u$ and $v$ with domain $D$. The set of all points $(x,y,z)$ in $\R^3$ such that $$x = x(u,v) \quad y = y(u,v) \quad z = z(u,v)$$ and $(u,v)$ varies throughout $D$ is called a \textbf{parametric surface} $S$ and the above equations are called parametric equations of $S$. Each choice of $u$ and $v$ gives a point on $S$; the surface of $S$ is traced out by the tip of the position vector $r(u,v)$ as $(u,v)$ varies in $D$.

\subsection{Surfaces of Revolution}

Surfaces of revolution can be represented parametrically. Let's consider the surface $S$ generated by rotating the curve $y = f(x)$, $a \leq x \leq b$, about the $x$-axis where $f(x) \geq 0$. Let $\theta$ be the angle of rotation. If $(x,y,z)$ is a point in $S$, then $$x=x \quad y = f(x)\cos{\theta} \quad z=f(x)\sin{x}$$ Therefore we take $x$ and $\theta$ to be parameters and regard the above equations as parametric equations of $S$. The parameter domain is given by $a \leq x \leq b$, $0 \leq \theta \leq 2\pi$.

We now find the tangent plane to a parametric surface $S$ traced out by a vector function $$r(u,v) = x(u,v)\hat{i} + y(u,v)\hat{j} + z(u,v)\hat{k}$$ at a point $P_0$ with position vector $r(u_0, v_0)$. If we keep $u$ constant by putting $u = u_0$, then $r(u_0, v)$ becomes a vector function of a single parameter $v$ and defines a grid curve $C_1$ lying on $S$. The tangent vector to $C_1$ at $P_0$ is obtained by taking the partial derivative of $r$ with respect to $v$: $$r_v = \frac{\partial x}{\partial v}(u_0, v_0)\hat{i} + \frac{\partial y}{\partial v}(u_0, v_0)\hat{j} + \frac{\partial z}{\partial v}(u_0, v_0)\hat{k} $$ Similarly, if we keep $v$ constant by setting $v = v_0$, we get a grid curve $C_2$ given by $r(u, v_0)$ that lies on $S$ and its tangent vector at $P_0$ is $$r_u = \frac{\partial x}{\partial u}(u_0, v_0)\hat{i} + \frac{\partial y}{\partial u}(u_0, v_0)\hat{j} + \frac{\partial z}{\partial u}(u_0, v_0)\hat{k}$$ If $r_u \times r_v \neq 0$, then the surface is called smooth. For a smooth surface, the tangent plane is the plane that contains the tangent vectors $r_u$ and $r_v$. The vector $r_u \times r_v$ is a normal vector to the tangent plane.

\subsection{Surface Area}

If a smooth parametric surface $S$ is given by the equation $$r(u,v) = x(u,v)\hat{i} + y(u,v)\hat{j} + z(u,v)\hat{k} \quad (u,v) \in D$$ and $S$ is covered just once as $(u,v)$ ranges throughout the parameter domain $D$, then the surface area of $S$ is $$A(S) = \iint\limits_D |r_y \times r_u| dA$$ where $$\begin{aligned}
    r_u =&\frac{\partial x}{\partial u}(u_0, v_0)\hat{i} + \frac{\partial y}{\partial u}(u_0, v_0)\hat{j} + \frac{\partial z}{\partial u}(u_0, v_0)\hat{k}\\
    r_v =&\frac{\partial x}{\partial v}(u_0, v_0)\hat{i} + \frac{\partial y}{\partial v}(u_0, v_0)\hat{j} + \frac{\partial z}{\partial v}(u_0, v_0)\hat{k}
\end{aligned}$$

\subsection{Surface Area of the Graph of a Function}

For the special case of a surface $S$ with equation $z = f(x,y)$ where $(x,y)$ lies in $D$ and $f$ has continuous partial derivatives, we can take $x$ and $y$ as parameters. The parametric equations are $$x = x \quad y = y \quad z = f(x,y)$$ so $$r_x = \hat{i} + \frac{\partial f}{\partial x}\hat{k} \quad r_y = \hat{j} + \frac{\partial f}{\partial y}\hat{k}$$ and $$r_x \times r_y = \begin{vmatrix}
    \hat{i} & \hat{j}  & \hat{k} \\
    1       & 0        & \frac{\partial f}{\partial x} \\
    0       & 1        & \frac{\partial f}{\partial y} \\
\end{vmatrix}$$ Thus we have $$|r_x \times r_y| = \sqrt{\Big( \frac{\partial f}{\partial x} \Big)^2 + \Big( \frac{\partial f}{\partial y} \Big)^2 + 1} = \sqrt{1 + \Big( \frac{\partial z}{\partial x} \Big)^2 + \Big( \frac{\partial z}{\partial y} \Big)^2}$$ and the surface area from the above definition becomes $$A(S) = \iint\limits_D = \sqrt{1 + \Big( \frac{\partial z}{\partial x} \Big)^2 + \Big( \frac{\partial z}{\partial y} \Big)^2} dA$$ This definition of surface area is consistent with the formula for surface area that we developed for solids of revolution in single-variable calculus.\footnote{p. 1119}

\subsection{Surface Integrals}

Suppose that a surface $S$ has a vector equation $$r(u,v) = x(u,v)\hat{i} + y(u,v)\hat{j} + z(u,v)\hat{k} \quad (u,v) \in D$$ Then we define \textbf{the surface integral of $f$ over $S$} as $$\iint\limits_S f(x,y,z) dS = \iint\limits_S f\big(r(u,v) \big) | r_u \times r_v | dA $$ Recall that $f\big(r(u,v)\big)$ is evaluated by pluggin in $x = x(u,v)$, $y = y(u,v)$, and $z = z(u,v)$.

If a thin sheet (of aluminum foil, for example) has the shape of a surface $S$ and the density of the sheet at $(x,y,z)$ is given by $\rho (x,y,z)$, then the total mass of the sheet is $$m = \iint\limits_S \rho (x,y,z) dS$$ and the center of mass is $(\overline{x}, \overline{y}, \overline{z})$ where $$\begin{aligned}
    \overline{x} =&\frac{1}{m}\iint\limits_S x \rho (x,y,z) dS\\
    \overline{y} =&\frac{1}{m}\iint\limits_S y \rho (x,y,z) dS\\
    \overline{z} =&\frac{1}{m}\iint\limits_S z \rho (x,y,z) dS
\end{aligned}$$

\subsection{Oriented Surfaces}

To deine surface integrals of vector fields, we need to rule out nonorientable surfaces such as the Möbius strip. From now on we only consider orientable (two-sided) surfaces. We start with a surface $S$ that has a tangent plane at every point $(x,y,z)$ on $S$ (except at a boundary point). There are two unit normal vectors $n_1$ and $n_2 = -n_1$ at $(x,y,z)$. If it's possible to choose a unit normal vector $n$ over every point on $S$, then $S$ is called an \textbf{oriented surface} and given the choice of $n$ provides $S$ with an \textbf{orientation}. There are two possible orientations for an orientable surface. For a surface $z = g(x,y)$, we associate with the surface a natural orientation given by the unit normal vector $$n = \frac{-\frac{\partial g}{\partial x}\hat{i} - \frac{\partial g}{\partial y}\hat{j} + \hat{k}}{\sqrt{1 + \Big( \frac{\partial g}{\partial x} \Big)^2 + \Big( \frac{\partial g}{\partial y} \Big)^2}}$$ Because the $k$-component is positive, this gives the upward orientation. If $S$ is a smooth orientable surface given in parametric form by a vector function $r(u,v)$, then it is automatically supplied with the orientation of the unit normal vector $$n = \frac{r_u \times r_v}{|r_u \times r_v|}$$ and the opposite orientation is given by $-n$.

For a \textbf{closed surface} (a surface that is the boundary for a solid region $E$), the convention is that the positive orientation is the one for which the normal vectors point \textit{outward} from $E$, and inward-pointing vectors give the negative orientation.

\subsection{Surface Integrals of Vector Fields}

Suppose that $S$ is an oriented surface with unit normal vector $n$, and imagine a fluid with density $\rho (x,y,z)$ and velocity field $v(x,y,z)$ flowing through $S$ (which in this case might be a fishing net or some other surface that doesn't impede fluid flow). The rate of flow (mass per unit time) is $\rho v$. The surface integral of the function $\rho v \cdot n$ over $S$ is: $$\iint\limits_S \rho v \cdot n dS = \iint\limits_S \rho (x,y,z) v(x,y,z) \cdot n(x,y,z) dS$$ which is interpreted as the rate of flow through $S$.

If we write $F = \rho v$, then $F$ is also a vector field on $\R^3$ and the above integral becomes $$\iint\limits_S F \cdot n dS$$ A surface integral of this form occurs frequently in physics, even when $F$ is not $\rho v$, and is called the surface (or flux) integral of $F$ over $S$, which we define below.

If $F$ is a continuous vector field defined over an oriented surface $S$ with normal vector $n$, then the \textbf{surface integral of $F$ over $S$} is $$\iint\limits_S F \cdot dS = \iint\limits_S F \cdot n \, dS$$ This integral is also called the \textbf{flux} of $F$ across $S$.

If $S$ is given by a vector function $r(u,v)$, then $n$ is given by one of the above equations and $$\begin{aligned}
    \iint\limits_S F \cdot dS&= \iint\limits_S F \cdot \frac{r_u \times r_v}{|r_u \times r_v|} dS\\
    &= \iint\limits_D \Big[ F\big(r(u,v)\big) \cdot \frac{r_u \times r_v}{|r_u \times r_v|} \Big] |r_u \times r_v| dA
\end{aligned}$$ where $D$ is the parameter domain. Thus $$\iint\limits_S F \cdot dS = \iint\limits_D F \cdot \big( r_u \times r_v \big) dA$$

In the case of $S$ given by a graph $z = g(x,y)$, we can think of $x$ and $y$ as parameters and write $$F \cdot \big( r_u \times r_v \big) = \big(P\hat{i} + Q\hat{j} + R\hat{j}\big) \cdot \Big(-\frac{\partial g}{\partial x}\hat{i} - \frac{\partial g}{\partial y}\hat{j} + k \Big)$$ thus the above formula becomes $$\iint\limits_S F \cdot dS = \iint\limits_D \Big(-P\frac{\partial g}{\partial x} - Q\frac{\partial g}{\partial y}\hat{j} + R \Big) dA$$ assuming upward orientation for $S$; for a downward orientation we multiply by $-1$. Similar formulas can be worked out if $x$ or $y$, rather than $z$, is given by a function of the other two variables.

\section{Stokes' Theorem}

\textbf{Stokes' Theorem} states that if $S$ is a piecewise-smooth surface bounded by a simple, closed, piecewise-smooth curve $C$ with positive orientation, and $F$ is a vector field whose components have continuous partial derivatives on an open region in $R^3$ containing $S$, then $$ \int_C F \cdot dr = \iint_S \Curl{F} \cdot dS$$ This theorem states that the line integral around the boundary curve of $S$ of the tangential component of $F$ is equal to the surface integral of $S$ of the normal component of the curl of $F$.

The positively oriented boundary curve of the oriented surface $S$ is often written as $\partial S$, so Stokes' Theorem can also be expressed as $$\iint_S \Curl{F} \cdot dS = \int_{\partial S} F \cdot dr$$

Stoke's Theorem can be regarded as a higher dimensional version of Green's Theorem. Whereas Green's theorem relates a double integral over a plane region $D$ to a line integral around its plane boundary curve, Stoke's Theorem relates a surface integral over a surface $S$ to a line integral around the boundary curve of $S$ (which is a space curve). The orientation of $S$ induces the positive orientation of the boundary curve $C$. This means if you walk in the positive direction around $C$ with your head pointing in the direction of $n$, then the surface will always be to your left.

In fact, the special case where $S$ is flat and lies on an $xy$-plane with upward orientation, the unit normal is $\hat{k}$, the surface integral becomes a double integral, and Stoke's Theorem becomes $$\int\limits_C F \cdot dr = \iint\limits_S \Curl{F} \cdot dS = \iint\limits_S (\Curl{F}) \cdot k \, dA$$

% TODO historical note

Sir George Gabriel Stokes (1819-1903)

William Thomson, 1st Baron Kelvin (1824-1907)

% Where did this come from?
% Stokes' Theorem generalizes Green's theorem to curves that are not bounded to planes. $$ \oint_C F \cdot dr = \iint_S \Curl{F} \cdot ds $$ where $C$ is not bound to a plane. This theorem states that the work done along a simple closed curve $C$ on $F$ is equal to the flux of $\Curl{F}$ across the surface bounded by $C$.

\section{The Divergence Theorem}

Recall Green's Theorem in terms of divergence: $$\oint\limits_C F \cdot ds = \iint\limits_D \Div{F(x,y)} \, dA$$ where $C$ is the positively oriented boundary curve of the plane region $D$. If we were seeking to extend this theorem to vector fields on $\R^3$, we might guess that $$\oint\limits_S F \cdot n \, dS = \iiint\limits_E \Div{F(x,y,z)} \, dV$$ where $S$ is the boundary surface of the solid region $E$. This is called The Divergence Theorem, and is true under the appropriate conditions. It relates the integral of a derivative of a function ($\Div F$ in this case) over a region to the integral of the original function $F$ over the boundary of the region.

We state The Divergence Theorem for regions that are simultaneously of types 1, 2, and 3, and call such regions \textbf{simple solid regions}. The boundary of $E$ is a closed surface, and we use the convention that the positive orientation of the unit vector $n$ is outward from $E$.

\textbf{The Divergence Theorem} Let $E$ be a simple solid region and let $S$ be the boundary surface of $E$, given with positive (outward) orientation. Let $F$ be a vector field whose component functions have continuous partial derivatives on an open region that contains $E$. Then The Divergence Theorem states that $$\iint\limits_C F \cdot \, dS = \iiint\limits_E \Div{F} dV $$ This theorem states that undr the given conditions, the flux of $F$ across the boundary surface of $E$ is equal to the triple integral of the divergence of $F$ over $E$.

The Divergence Theorem states that, under the given conditions, the flux of $F$ across the boundary surface of $E$ is equal to the triple integral of the divergence of $F$ over $E$.

\section{Generalized Stokes Theorem}

The generalized Stokes Theorem, also called the Stokes-Cartan theorem\footnote{From the famed mathematician Élie Cartan (1869-1951), who gave stated the generalized Stokes' Theorem in 1945.}, Stokes theorem says that the integral of a differential form $\omega$ over the boundary of some orientable manifold $\Omega$ is equal to the integral of its exterior derivative $d\omega$ over $\Omega$:

$$\int_{\partial \Omega}\omega = \int_{\Omega} d\omega $$

Green's Theorem, the classical Stokes Theorem, and Divergence Theorem are special cases of the generalized Stokes' Theorem. The generalized Stokes theorem is also a generalization of the fundamental theorem of calculus.

\pagebreak \section{Miscellaneous}

A \textbf{closed set} is one that contains all its boundary points.

The vector representation of a line segment that starts at $r_{0}$ and ends at $r_{1}$ is $r(t) = (1-t)r_{0} + tr_{1}$.

$$\hat{i} = \begin{bmatrix}
1\\
0\\
0
\end{bmatrix},
\quad
\hat{j} = \begin{bmatrix}
0\\
1\\
0
\end{bmatrix},
\quad
\hat{k} = \begin{bmatrix}
0\\
0\\
1
\end{bmatrix}$$

$$\hat{u} = \cos{\theta}\hat{i} + \sin{\theta}\hat{j}$$

Equations for cylinders have only 2 variables. These equations give a trace of the curve on the coordinate plane denoted by the given variables. The curve is directed along the axes of the missing variables.

Frenet-Serret Frames: at every point, the TNB frame gives...

$n$-variable functions are surfaces in $\mathbb{R}^{n}$, and the derivative of this function represents the equation of the tangent plane to the function at each point

\textbf{Derivation. (Directional Derivative)}

We have $f$ and $\vec{v}$, and a point $P$.

First, make a line through $P || \vec{u}$ (parallel to $\vec{u}$).

Extend the line upwards and downwards to create a plane parallel to the $\hat{u}-z$ plane, creating a curve on the surface of the plane. (Imagine the plane like a knife used to create a cross-section.)

By projecting the point $P$ onto the function, we get the point $P'$. By choosing a second point $Q$ somewhere else along $\hat{u}$ and projecting it onto the function, we get a second point $Q'$.

$P$ is analogous to $x_{1}$, $Q = x_{2}$, $P' = y_{1}$, $Q' = y_{2}$ in the rise-over-run difference quotient.

$$D_{\hat{u}} f(x,y) = \lim_{\substack{\Delta x \to 0 \\ \Delta y \to 0}} \frac{f(x + \Delta x, y + \Delta y) - f(x,y)}{\sqrt{\Delta x^{2} + \Delta y^{2}}}$$

$\overline{PQ} || \hat{u} \rightarrow \overline{PQ} = h \times \hat{u}$

$\overline{PQ} = \Delta x \hat{i} + \Delta y \hat{i} = h \hat{u}_{1} \hat{i} + h \hat{u}_{2} \hat{j}$

Letting $\Delta x = h \hat{u}_{1}$ and $\Delta y = h \hat{u}_{2}$, we see that $\sqrt{\Delta x^{2} + \Delta y^{2}} = \sqrt{h^{2} \hat{u}_{1}^{2} + h^{2} \hat{u}_{2}^{2}} = h \times \sqrt{\hat{u}_{1}^{2} + \hat{u}_{2}^{2}} = h$ (because $\hat{u}$ is a unit vector), and can rewrite the difference quotient as

$$D_{\hat{u}} f(x,y) = \lim_{h \to 0} \frac{f(x + h\hat{u}_{1}, y + h\hat{u}_{2}) - f(x,y)}{h}$$

$D_{\hat{u}} f(x,y) = u_{1}f_{x} + u_{2}f_{y}$

\end{document}